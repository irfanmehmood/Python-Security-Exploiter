import time
import logging

# Scrapy
import scrapy
from scrapy import Spider
from scrapy.http import Request, FormRequest
from scrapy.crawler import CrawlerProcess
from scrapy.utils.response import open_in_browser
from scrapy.spiders.init import InitSpider
from scrapy.spiders import CrawlSpider, Rule
from scrapy.linkextractors import LinkExtractor

# Logging
logging.getLogger('scrapy').setLevel(logging.CRITICAL)
logging.getLogger('scrapy').propagate = False

# IMPORT OUR CUSTOM CLASSES/LIBS
##############################################################################################

import config
from Utility.utility import Utility
from Db.db import Db
from Exploits import Exploits

# INITIALISE OUR CLASSES/LIBS/GLOBALS
##############################################################################################

U = Utility()
Db = Db()
JOB_URL = config.host['JOB_URL']
JOB_LOGIN_URL = config.host['JOB_LOGIN_URL']
DOMAIN = U.host_from_url(JOB_URL)
SECONDS_BETWEEN_REQUESTS = config.host['SECONDS_BETWEEN_REQUESTS']
USERNAME = config.host['USERNAME']
PASSWORD = config.host['PASSWORD']
# Lits of exploits available for this program
EXPLOITS_LIST = Exploits.load_exploits()
# Pages fetched from the database, after they have been verified to have contained exploits are marked as exploitable
PAGES_ATTACKED_SUCCESSFULLY = []
# List if pages from the database
DB_FETECHED_PAGES_ATTACK_FOR_EXPLOITS = []
CREATED_URL_PAYLOADS = []
EXCLUDE_PAGES = ['logout.php', 'signout']
SUCCESS_COUNT = 0


#############################################################################################
class AuthAttacker(CrawlSpider):

    name = 'Crawler'
    login_page = JOB_LOGIN_URL
    Domain = U.host_from_url(JOB_URL)
    allowed_domains = [Domain]
    start_urls = [JOB_LOGIN_URL]

    # Send our login request
    def start_requests(self):
        yield Request(url=self.login_page, callback=self.send_credentials, dont_filter=True)

    # Send our credentials for authorisation, when server requests
    def send_credentials(self, response):
        U.cprint('Sending credentials', 'OKBLUE')
        login_data = {'username': USERNAME, 'password': PASSWORD}
        
        return FormRequest.from_response(response, formdata=login_data, callback=self.server_login_response)

    # Response from server after sending our credentials
    def server_login_response(self, response):
        U.cprint('Check Login respose.', 'OKBLUE', True)

        # Check iof we are logged in
        if "Logout" in response.body:
            U.cprint('Successfully logged in.', 'OKGREEN', True)

            # We have to manually create request for the spider to work
            PAGE_REQUESTS = []

            for url in CREATED_URL_PAYLOADS:
                #print(url)
                PAGE_REQUESTS.append(Request(url, callback=self.parse_received_page))
                    
            return PAGE_REQUESTS
        else:
            # Login failed. Something went wrong.
            U.cprint("Login failed. Something went wrong.", 'FAIL', True)


    # Parse our response after we have logged in successfully
    def parse_received_page(self, response):

        path = U.path_from_url(response.request.url)
        domain = U.host_from_url(response.request.url)

        for exploits in EXPLOITS_LIST:
            found = exploits.did_attack_work(response.body)
            if  found:
                U.cprint("Exploit Worked: [ " +  exploits.__class__.__name__ + ']', 'BOLD', False)
                U.cprint("Page : [ " + domain + "/" + path + ']', 'OKGREEN', False)
                U.cprint("Fruits: [" + str(found) + "]", 'BOLD', True)
                SUCCESS_COUNT = SUCCESS_COUNT + 1
        
        
# START SCRIPT
##############################################################################################

JOB = Db.job_get(JOB_URL)
if (JOB):
    JOB_ID = str(JOB['_id'])
    U.cprint_show_job(JOB)
    if JOB['attack_finished'] == 1:
        U.cprint('JOB HAS BEEN ATTACKED', 'WARNING', True)
    else:
        # Good to go and start the script
        DB_FETECHED_PAGES_ATTACK_FOR_EXPLOITS = Db.page_get_attackable(JOB_ID)
        found = len(DB_FETECHED_PAGES_ATTACK_FOR_EXPLOITS)

        if found == 0:
            U.cprint('JOB: ATTACK: NO URLS FOR THIS JOB', 'FAIL', True)
        else:
            U.cprint('JOB ATTACKING [' + str(found) +
                        "] PAGES WITH [" + str(len(EXPLOITS_LIST)) + "] EXPLOITS EACH", 'OKGREEN', False, False)
            U.hr(False)
            for page in DB_FETECHED_PAGES_ATTACK_FOR_EXPLOITS:
                # We are only interested in pages with status code 200
                if page['status_code'] == 200:
                    # For each exploit in our List, we call its process method on the html
                    for exploits in EXPLOITS_LIST:
                        full_url = exploits.create_payload_url(page['exploit_payloads_found'], page['page_full_path'])
                        if full_url:
                            CREATED_URL_PAYLOADS.append(full_url)
            
            process = CrawlerProcess()
            process.crawl(AuthAttacker)
            process.start()  # the script will block here until all crawling jobs are finished


            
else:
    U.cprint('NO JOB EXIST: Job does not exists in the Database.', 'FAIL')
    U.cprint('JOB URL: ' + JOB_URL, 'FAIL', True)


# Finish
#######################################################################

msg = ("JOB ATTACK FINISHED: PAGES ATTACKED:[" 
        + str(len(DB_FETECHED_PAGES_ATTACK_FOR_EXPLOITS) * len(EXPLOITS_LIST)) + "]"
        + " SUCCESSFULL ATTACKS:[" + str(SUCCESS_COUNT) + "]")
U.hr(False)
U.cprint(msg, 'OKGREEN', False, False)
U.hr(False)