

# IMPORT OUR CUSTOM CLASSES/LIBS
##############################################################################################

import config
from Utility.utility import Utility
from Db.db import Db
from Exploits import Exploits

# INITIALISE OUR CLASSES/LIBS/VARS
##############################################################################################

U = Utility()
Db = Db()
JOB_URL = config.host['JOB_URL']

#Lits of exploits available for this program
EXPLOITS_LIST = Exploits.load_exploits()

# Pages fetched from the database, after they have been verified to have contained exploits are marked as exploitable
PAGES_FOUND_WITH_EXPLOITS = []

# List if pages from the database
DB_FETECHED_PAGES_CHECK_FOR_EXPLOITS = []

# START SCRIPT
##############################################################################################
# U.cls()
# U.show_crawling_screen()
JOB = Db.job_get(JOB_URL)
if (JOB):
    JOB_ID = str(JOB['_id'])
    U.cprint_show_job(JOB)
    if JOB['scan_finished'] == 1:
        U.cprint('JOB HAS BEEN ANALYSED', 'WARNING', False, False)
        U.hr(False)
    else:
        # Good to go and start the script
        DB_FETECHED_PAGES_CHECK_FOR_EXPLOITS = Db.page_get_unscanned(JOB_ID)
        found = len(DB_FETECHED_PAGES_CHECK_FOR_EXPLOITS)

        if found == 0:
            U.cprint('JOB: ANAYLYSING: NO URLS FOR THIS JOB', 'FAIL', True)
        else:
            U.cprint('JOB ANALYSIS START [' + str(found) +
                     "] PAGES", 'OKGREEN', False, False)
            U.hr(False)

        # START VERIFYING PAGES AGAINTS ALL EXPLOITS
        #######################################################################################
        for page in DB_FETECHED_PAGES_CHECK_FOR_EXPLOITS:
           
            # We are only interested in pages with status code 200
            if page['status_code'] == 200:
                
                # For each exploit in our List, we call its process method on the html
                for exploits in EXPLOITS_LIST:

                    # Process the page with 
                    exploit_payloads_found = exploits.check_if_exploit_found(page['page_html'])
                
                    if exploit_payloads_found == False:
                        U.cprint("Nothing Found in : " + page['page_path'], 'FAIL', True)
                        
                    else:
                        #Great we found something, add this page for attacking
                        page['exploit_payloads_found'] = exploit_payloads_found
                        page['exploits_found'] = 1
                        PAGES_FOUND_WITH_EXPLOITS.append(page)
                        U.cprint("Found Something in : " + page['page_path'], 'OKGREEN', True)
                
                #This page has been scanned now, set that in DB, so its cleared from the queue
                Db.page_set_scanned(page['_id'])

else:
    U.cprint('NO JOB EXIST: Job does not exists in the Database.', 'FAIL')
    U.cprint('JOB URL: ' + JOB_URL, 'FAIL', True)


#Finish
#######################################################################
for page in PAGES_FOUND_WITH_EXPLOITS:
    try :
        Db.page_payloads_found(page)
    except:
        print "Something wrong with this page"
    U.cprint("Updating in database : " + page['page_path'], 'WARNING', True)

msg = ("JOB ANALYSIS FINISHED: PAGES ANALYSED:[" + str(len(DB_FETECHED_PAGES_CHECK_FOR_EXPLOITS)) + "]"
                + " PAGES EXPLOITABLE:[" + str(len(PAGES_FOUND_WITH_EXPLOITS)) + "]")
U.hr(False)
U.cprint(msg, 'OKGREEN', False, False)
U.hr(False)
Db.job_scan_finnished(JOB_URL)