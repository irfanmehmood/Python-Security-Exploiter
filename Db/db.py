import os
import config
from pymongo import MongoClient

class Db:

    # node which is connecting, setup for multiple
    nodeIP = ''
    DbConnect = False

    def __init__(self):
        # node unique ip, which is connecting
        self.nodeIP = config.node['IP']
        # This should be the centeral db server
        client = MongoClient(config.node['DB_CONNECTION'])
        self.DbConnect = client.Exploiter

    def page_add(self, job_id, job_url, status_code, page_html, page_path, page_full_path):
        return self.DbConnect.page.insert({
            "job_id" : job_id,
            'job_url' : job_url,
            "status_code" : status_code,
            "page_html" : page_html,
            "page_path": page_path,
            "page_full_path" : page_full_path,
            "attack_complete": 0,
            "scan_complete": 0,
            "exploits_found": 0,
            "exploit_payloads_found" : '',
            "exploit_payload_response" : '',
            "exploit_payload_response_mini" : '',
            "node_ip": self.nodeIP
        })

    def page_add_collected_links(self, internal_or_external, url, page_id, job_id):
        self.DbConnect.page_collected_links.insert({
            "job_id" : job_id,
            'page_id' : page_id,
            "url": url,
            # Internal = 1, external = 0
            "internal_or_external": internal_or_external,
            "node_ip": self.nodeIP
        })

    def job_create(self, job_url, job_login_url):
        #returns an _id after insert
        return self.DbConnect.job.insert({
            "job_login_url": job_login_url,
            "job_url": job_url,
            "scan_finished": 0,
            "crawl_finished": 0,
            "attack_finished": 0,
            "node_ip": self.nodeIP
        })

    def job_get(self, job_url):
        q = {"job_url": job_url, "node_ip": self.nodeIP}
        result = self.DbConnect.job.find_one(q)
        return result

    def page_get_unscanned(self, job_id):
        find = {
            "job_id" : job_id,
            "node_ip": self.nodeIP, 
            "scan_complete": 0}
        return list(self.DbConnect.page.find(find))

    def page_get_attackable(self, job_id):
        find = {
            "job_id" : job_id,
            "node_ip": self.nodeIP,
            "attack_complete" : 0,
            "exploits_found": 1}
        return list(self.DbConnect.page.find(find))

    def page_set_scanned(self, _id):
        query = {"_id": _id, "node_ip": self.nodeIP}
        update_vals = {'$set': {"scan_complete": 1}}
        self.DbConnect.page.update_one(query, update_vals)
    
    def page_exists(self, page_path):
        find = {"page_path": page_path, "node_ip": self.nodeIP}
        count = self.DbConnect.page.find(find).count()
        return False if int(count) == 0 else True

    def page_payloads_found(self, page):
        query = {"_id": page['_id'], "node_ip": self.nodeIP}
        update_vals = {'$set': {"exploits_found": 1, "exploit_payloads_found" : page['exploit_payloads_found']}}
        self.DbConnect.page.update_one(query, update_vals)

    def job_scan_finnished(self, job_url):
        query = {"job_url": job_url, "node_ip": self.nodeIP}
        update_vals = {'$set': {"scan_finished": 1}}
        self.DbConnect.job.update_one(query, update_vals)

    def job_crawl_finished(self, job_url):
        query = {"job_url": job_url, "node_ip": self.nodeIP}
        update_vals = {'$set': {"crawl_finished": 1}}
        self.DbConnect.job.update_one(query, update_vals)

    def job_attack_finished(self, job_url):
        query = {"job_url": job_url, "node_ip": self.nodeIP}
        update_vals = {'$set': {"attack_finished": 1}}
        self.DbConnect.job.update_one(query, update_vals)

    def stats_group_by_path(self):
        agr = [{'$group': {'_id' : '$path', 'count': {'$sum': 1}}}]
        val = list(self.DbConnect.url_exploit_response.aggregate(agr))
        return val

    def stats_get_succesfull_exploits(self):
        results = list(self.DbConnect.url_exploit_response.find())
        return results

    def jobs_clear_all_data(self):
        self.DbConnect.job.remove({})
        self.DbConnect.page.remove({})
        self.DbConnect.page_collected_links.remove({})
        self.DbConnect.page_payloads.remove({})


# show dbs
# show databaseName
# use crawlerDb;
# use file_to_scan
