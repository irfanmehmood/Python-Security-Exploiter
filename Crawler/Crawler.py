import time
import logging

# Scrapy
import scrapy
from scrapy import Spider
from scrapy.http import Request, FormRequest
from scrapy.crawler import CrawlerProcess
from scrapy.utils.response import open_in_browser
from scrapy.spiders.init import InitSpider
from scrapy.spiders import CrawlSpider, Rule
from scrapy.linkextractors import LinkExtractor

# Logging
logging.getLogger('scrapy').setLevel(logging.CRITICAL)
logging.getLogger('scrapy').propagate = False

# IMPORT OUR CUSTOM CLASSES/LIBS
##############################################################################################

import config
from Utility.utility import Utility
from Db.db import Db

# INITIALISE OUR CLASSES/LIBS/GLOBALS
##############################################################################################

U = Utility()
Db = Db()
JOB_URL = config.host['JOB_URL']
JOB_LOGIN_URL = config.host['JOB_LOGIN_URL']
DOMAIN = U.host_from_url(JOB_URL)
SECONDS_BETWEEN_REQUESTS = config.host['SECONDS_BETWEEN_REQUESTS']
USERNAME = config.host['USERNAME']
PASSWORD = config.host['PASSWORD']
GLOBAL_PAGES_ALREADY_SCRAPPED = {}
GLOBAL_UNIQUE_INTERNAL_LINKS = []
GLOBAL_UNIQUE_EXTERNAL_LINKS = []
GLOBAL_INTERNAL_404_PAGES = []
EXCLUDE_PAGES = ['logout.php', 'signout']

#############################################################################################
class AuthCrawler(CrawlSpider):

    name = 'Crawler'
    login_page = JOB_LOGIN_URL
    Domain = U.host_from_url(JOB_URL)
    allowed_domains = [Domain]
    start_urls = [JOB_LOGIN_URL]

    # This spider has one rule: extract all (unique and canonicalized) links, follow them and parse them using the parse_items method
    rules = [
        Rule(
            LinkExtractor(
                canonicalize=True,
                unique=True,
                allow=('/vulnerabilities'),
            ),
            # follow is a boolean which specifies if links should be followed from each response extracted with this rule. 
            # If callback is None follow defaults to True, otherwise it defaults to False.
            follow=True,
            callback="parse_received_page"
        )
    ]

    # Send our login request
    def start_requests(self):
        yield Request(url=self.login_page, callback=self.send_credentials, dont_filter=True)

    # Send our credentials for authorisation, when server requests
    def send_credentials(self, response):
        U.cprint('Sending credentials', 'OKBLUE')
        login_data = {'username': USERNAME, 'password': PASSWORD}
        
        return FormRequest.from_response(response, formdata=login_data, callback=self.server_login_response)

    # Response from server after sending our credentials
    def server_login_response(self, response):
        U.cprint('Check Login respose.', 'OKBLUE', True)

        # Check iof we are logged in
        if "Logout" in response.body:
            U.cprint('Successfully logged in.', 'OKGREEN', True)

            # We have to manually create request for the spider to work
            PAGE_REQUESTS = []

            # Extract all links from current page
            for Exploiter in response.css('#main_menu_padded ul li'):
                url = Exploiter.css('a::attr(href)').extract()
                # We remove the trailing slashes and period , that polutes path
                path = str(url[0]).rstrip('.').strip("/")
                full_url =  str(JOB_URL) + path

                #Exclude any logouts as that will end our session
                if path not in EXCLUDE_PAGES:
                    PAGE_REQUESTS.append(Request(full_url, callback=self.parse_received_page))
                    
            return PAGE_REQUESTS
        else:
            # Login failed. Something went wrong.
            U.cprint("Login failed. Something went wrong.", 'FAIL', True)


    # Parse our response after we have logged in successfully
    def parse_received_page(self, response):
        
        status_color = 'FAIL' if str(response.status) == '404' else 'OKBLUE'

        if DOMAIN == U.host_from_url(response.request.url) and response.request.url not in GLOBAL_PAGES_ALREADY_SCRAPPED:

            # Only extract canonicalized and unique links (with respect to the current page)
            links_found_on_this_page = LinkExtractor(canonicalize=True, unique=True).extract_links(response)              
            
            INTERNAL_LINKS_IN_THIS_PAGE = []
            EXTERNAL_LINKS_IN_THIS_PAGE = []
            
            for link in links_found_on_this_page:
                
                # Check if it is an internal link
                isLinkInternal = DOMAIN == U.host_from_url(link.url)

                if isLinkInternal:

                    if link.url not in INTERNAL_LINKS_IN_THIS_PAGE:
                        INTERNAL_LINKS_IN_THIS_PAGE.append(link.url)

                    #Add to global links, collected.
                    if link.url not in GLOBAL_UNIQUE_INTERNAL_LINKS:
                        GLOBAL_UNIQUE_INTERNAL_LINKS.append(link.url)
                        time.sleep(0.5)
                        U.cprint('New Link, Make Request: ' + link.url, 'WARNING', True)
                        yield (Request(link.url, callback=self.parse_received_page))

                else:
                    #Its an external domain
                    EXTERNAL_LINKS_IN_THIS_PAGE.append(link.url)

                    if link.url not in EXTERNAL_LINKS_IN_THIS_PAGE:
                        EXTERNAL_LINKS_IN_THIS_PAGE.append(link.url)

                    #Add to global links, collected.
                    if link.url not in GLOBAL_UNIQUE_EXTERNAL_LINKS:
                        GLOBAL_UNIQUE_EXTERNAL_LINKS.append(link.url)


            U.cprint ("PAGE: [" + U.path_from_url(response.request.url) + "]", status_color)
            msg = ("LINKS:[" + str(len(links_found_on_this_page)) + "]"
                + " INTERNAL:[" + str(len(INTERNAL_LINKS_IN_THIS_PAGE)) + "]"
                + "-UNIQUE:[" + str(len(INTERNAL_LINKS_IN_THIS_PAGE)) + "]"
                + "  EXTERNAL:[" + str(len(EXTERNAL_LINKS_IN_THIS_PAGE)) + "]"
                + "-UNIQUE:[" + str(len(EXTERNAL_LINKS_IN_THIS_PAGE)) + "]")

            U.cprint (msg, 'OKGREEN')
            U.cprint ("URL: " + response.request.url, status_color)
            U.hr()

            # Lets add our new page to the database
            #########################################################################
            GLOBAL_PAGES_ALREADY_SCRAPPED[response.request.url] = {
                "status_code" : response.status,
                "page_html" : response.body,
                "page_path": U.path_from_url(response.request.url),
                "page_full_url" : response.request.url,
                "external_links_list" : GLOBAL_UNIQUE_EXTERNAL_LINKS,
                "internal_links_list" : GLOBAL_UNIQUE_INTERNAL_LINKS
            }

            #for link in INTERNAL_LINKS_IN_THIS_PAGE:
                #print link

            #print (GLOBAL_PAGES_ALREADY_SCRAPPED)




##############################################################################################
class NonAuthCrawler(CrawlSpider):
    # The name of the spider
    name = "datablogger"

    # The domains that are allowed (links to other domains are skipped)
    Domain = U.host_from_url(JOB_URL)
    allowed_domains = [Domain]
    start_urls = [JOB_URL]
    base_url = JOB_URL
    handle_httpstatus_list = [404]

    # This spider has one rule: extract all (unique and canonicalized) links, follow them and parse them using the parse_items method
    rules = [
        Rule(
            LinkExtractor(
                canonicalize=True,
                unique=True,
                #allow=('about-us/'),
            ),
            # follow is a boolean which specifies if links should be followed from each response extracted with this rule. 
            # If callback is None follow defaults to True, otherwise it defaults to False.
            follow=True,
            callback="parse_received_page"
        )
    ]

    def parse_received_page(self, response):

        # Check, Incase the received scarpped page is not the same domain as the domian we are scrapping
        # Check also if we have not scrapped this page already 

        status_color = 'FAIL' if str(response.status) == '404' else 'OKBLUE'

        if DOMAIN == U.host_from_url(response.request.url) and response.request.url not in GLOBAL_PAGES_ALREADY_SCRAPPED:

            # Only extract canonicalized and unique links (with respect to the current page)
            links_found_on_this_page = LinkExtractor(canonicalize=True, unique=True).extract_links(response)              
            
            INTERNAL_LINKS_IN_THIS_PAGE = []
            EXTERNAL_LINKS_IN_THIS_PAGE = []
            
            for link in links_found_on_this_page:
                
                # Check if it is an internal link
                isLinkInternal = DOMAIN == U.host_from_url(link.url)

                if isLinkInternal:

                    if link.url not in INTERNAL_LINKS_IN_THIS_PAGE:
                        INTERNAL_LINKS_IN_THIS_PAGE.append(link.url)

                    #Add to global links, collected.
                    if link.url not in GLOBAL_UNIQUE_INTERNAL_LINKS:
                        GLOBAL_UNIQUE_INTERNAL_LINKS.append(link.url)

                else:
                    #Its an external domain
                    EXTERNAL_LINKS_IN_THIS_PAGE.append(link.url)

                    if link.url not in EXTERNAL_LINKS_IN_THIS_PAGE:
                        EXTERNAL_LINKS_IN_THIS_PAGE.append(link.url)

                    #Add to global links, collected.
                    if link.url not in GLOBAL_UNIQUE_EXTERNAL_LINKS:
                        GLOBAL_UNIQUE_EXTERNAL_LINKS.append(link.url)

            U.cprint ("PAGE: [" + U.path_from_url(response.request.url) + "]", status_color)
            msg = ("LINKS:[" + str(len(links_found_on_this_page)) + "]"
                + " INTERNAL:[" + str(len(INTERNAL_LINKS_IN_THIS_PAGE)) + "]"
                + "-UNIQUE:[" + str(len(INTERNAL_LINKS_IN_THIS_PAGE)) + "]"
                + "  EXTERNAL:[" + str(len(EXTERNAL_LINKS_IN_THIS_PAGE)) + "]"
                + "-UNIQUE:[" + str(len(EXTERNAL_LINKS_IN_THIS_PAGE)) + "]")

            U.cprint (msg, 'OKGREEN')
            U.cprint ("URL: " + response.request.url, status_color)
            U.hr()

            # Lets add our new page to the database
            #########################################################################
            GLOBAL_PAGES_ALREADY_SCRAPPED[response.request.url] = {
                "status_code" : response.status,
                "page_html" : response.body,
                "page_path": U.path_from_url(response.request.url),
                "page_full_url" : response.request.url,
                "external_links_list" : GLOBAL_UNIQUE_EXTERNAL_LINKS,
                "internal_links_list" : GLOBAL_UNIQUE_INTERNAL_LINKS
            }

# Crawling Start
##############################################################################################
JOB = Db.job_get(JOB_URL)
if (JOB):
    JOB_ID = str(JOB['_id'])
    U.cprint_show_job(JOB)
    if JOB['crawl_finished'] == 1:
        U.cprint('JOB HAS BEEN CRAWLED', 'WARNING', False, False)
        U.hr(False)
    else:
        # Good to go and start the script
        U.cprint('JOB CRAWLING START', 'OKGREEN', False, False)
        U.hr(False)
        process = CrawlerProcess()

        if  "" == JOB_LOGIN_URL:
            process.crawl(NonAuthCrawler)
        else:
            process.crawl(AuthCrawler)

        process.start()  # the script will block here until all crawling jobs are finished
else:
    U.cprint('NO JOB EXIST: Job does not exists in the Database.', 'FAIL')
    U.cprint('DOMAIN URL: ' + JOB_URL, 'FAIL', True)

# Crawling Finished
#################################################################################################

# Add our crawled pages in the database
for page in GLOBAL_PAGES_ALREADY_SCRAPPED:

    row = GLOBAL_PAGES_ALREADY_SCRAPPED[page]
    page_path = str(row['page_path'])
    if Db.page_exists(page_path):
        U.cprint ("PAGE EXITS: " + page_path, 'WARNING')
    else:
        
        body = str(row['page_html'])
        page_full_url = str(row['page_full_url'])
        internal_links_list = row['internal_links_list']
        external_links_list = row['external_links_list']
        status_code = row['status_code']

        #Add our page and get its _id
        page_id = Db.page_add(JOB_ID, JOB_URL, status_code, body, page_path, page_full_url)
        U.cprint ("ADDED: " + page_path, 'OKBLUE')
        # Add Internal Links
        for int_link in internal_links_list:
            Db.page_add_collected_links(1, str(int_link), str(page_id), JOB_ID)
            #print (int_link)
        # Add Exxternal Links
        for ext_link in external_links_list:
            Db.page_add_collected_links(0, str(ext_link), str(page_id), JOB_ID)
            #print (ext_link)

#Update Job Crawling Status as Completed
Db.job_crawl_finished(JOB_URL)

U.hr(False)
msg = ("JOB CRAWLING FINISHED SCAPPED:[" + str(len(GLOBAL_PAGES_ALREADY_SCRAPPED)) + "]"
        + " INTERNAL:[" + str(len(GLOBAL_UNIQUE_INTERNAL_LINKS)) + "]"
        + " EXTERNAL:[" + str(len(GLOBAL_UNIQUE_EXTERNAL_LINKS)) + "]"
        + " ERROR PAGE:[" + str(len(GLOBAL_INTERNAL_404_PAGES)) + "]")


U.cprint(msg, 'OKGREEN', False, False)
U.hr(False)